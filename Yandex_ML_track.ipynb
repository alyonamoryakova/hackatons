{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fastText\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse \n",
    "import pandas as pd, numpy as np\n",
    "#import xgboost\n",
    "#import gensim\n",
    "#from gensim.models.wrappers import FastText\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import scipy.sparse as sps\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import cross_val_score , GroupKFold, StratifiedKFold\n",
    "import re\n",
    "#from xgboost import XGBRegressor\n",
    "import string\n",
    "#import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yandex ML track \n",
    "https://contest.yandex.ru/algorithm2018/contest/7914 \n",
    "Подбор реплик для продолжения диалога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fastText' from '/usr/local/lib/python3.6/dist-packages/fastText/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fastText.load_model(\"data/model.bin\")\n",
    "#ft_model_en = fastText.load_model(\"data/model-en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.tsv', sep='\\t', quotechar=' ', header = None)\n",
    "train.columns = ['context_id', 'context_2', 'context_1', 'context_0', 'reply_id', 'reply', 'label', 'confidence']\n",
    "test = pd.read_csv('data/public.tsv', sep='\\t', quotechar = ' ', header = None)\n",
    "test.columns = ['context_id', 'context_2', 'context_1', 'context_0', 'reply_id', 'reply']\n",
    "final = pd.read_csv('data/final.tsv', sep='\\t', quotechar = ' ', header = None)\n",
    "final.columns = ['context_id', 'context_2', 'context_1', 'context_0', 'reply_id', 'reply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_enc(x ,reverse = False):\n",
    "    if reverse == False:\n",
    "        if x == 'bad':\n",
    "            return 0\n",
    "        elif x == 'neutral':\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    else:\n",
    "        if x == 0:\n",
    "            return 'bad'\n",
    "        elif x == 1:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'good'\n",
    "        \n",
    "def get_context_number(row):\n",
    "    if pd.isnull(row['context_1']):\n",
    "        return 1\n",
    "    elif pd.isnull(row['context_2']):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "        \n",
    "        \n",
    "        \n",
    "train['label'] = train['label'].apply(lambda x: label_enc(x))\n",
    "\n",
    "train['context_number'] = train.apply(lambda x: get_context_number(x), axis = 1)\n",
    "test['context_number'] = test.apply(lambda x: get_context_number(x), axis = 1)\n",
    "    \n",
    "train.fillna('', inplace=True)\n",
    "test.fillna('', inplace=True)\n",
    "\n",
    "final['context_number'] = final.apply(lambda x: get_context_number(x), axis = 1)\n",
    "final.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna('', inplace=True)\n",
    "test.fillna('', inplace=True)\n",
    "final.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_punctuation(x):\n",
    "    '''\n",
    "        x : string\n",
    "            String from which the punctuation is removed\n",
    "    '''\n",
    "    return re.sub('[%s]' % string.punctuation, ' ', x)\n",
    "\n",
    "train['reply_without_punct'] = train['reply'].apply(lambda x: drop_punctuation(x))\n",
    "train['reply_without_punct'] = train['reply_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "train['reply_arr'] = train['reply_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "\n",
    "test['reply_without_punct'] = test['reply'].apply(lambda x: drop_punctuation(x))\n",
    "test['reply_without_punct'] = test['reply_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "test['reply_arr'] = test['reply_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def get_normal_form(row):\n",
    "    s = ''\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        s += p.normal_form + ' '\n",
    "    return s\n",
    "\n",
    "def check_if_verb(row):\n",
    "    f = 0\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if 'VERB' in p.tag:\n",
    "            f =1\n",
    "            \n",
    "    return f\n",
    "\n",
    "def clear(row):\n",
    "    s = ''\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if ( 'LATN' in p.tag) and ('PNCT'  in p.tag) and ('NUMB'  in p.tag) and ('intg' in p.tag) and ('real'  in p.tag) and ('ROMN'  in p.tag) and ( 'UNKN'  in p.tag):\n",
    "            s += x + ' '\n",
    "\n",
    "train['normal_reply'] = train['reply_arr'].apply(lambda x: get_normal_form(x))\n",
    "    \n",
    "train['reply'] = train['reply_arr'].apply(lambda x: clear(x))\n",
    "test['reply'] = test['reply_arr'].apply(lambda x: clear(x))\n",
    "\n",
    "train['normal_reply'] = train['reply_arr'].apply(lambda x: get_normal_form(x))\n",
    "\n",
    "#train['reply_contains_verb'] = train['reply_arr'].apply(lambda x: check_if_verb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['reply_without_punct'] = final['reply'].apply(lambda x: drop_punctuation(x))\n",
    "final['reply_without_punct'] = final['reply_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "final['reply_arr'] = final['reply_without_punct'].str.split(\" \").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>context_2</th>\n",
       "      <th>context_1</th>\n",
       "      <th>context_0</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>context_number</th>\n",
       "      <th>reply_without_punct</th>\n",
       "      <th>reply_arr</th>\n",
       "      <th>normal_reply</th>\n",
       "      <th>reply_contains_verb</th>\n",
       "      <th>reply_contains_noun</th>\n",
       "      <th>reply_contains_advb</th>\n",
       "      <th>reply_contains_numr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22579918886</td>\n",
       "      <td>кликни на меня а потом на надпись \" видео - зв...</td>\n",
       "      <td>о , я тебя вижу .</td>\n",
       "      <td>ладно , повесь трубку .</td>\n",
       "      <td>0</td>\n",
       "      <td>не могу .</td>\n",
       "      <td>2</td>\n",
       "      <td>0.875352</td>\n",
       "      <td>3</td>\n",
       "      <td>не могу</td>\n",
       "      <td>[не, могу]</td>\n",
       "      <td>не мочь</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22579918886</td>\n",
       "      <td>кликни на меня а потом на надпись \" видео - зв...</td>\n",
       "      <td>о , я тебя вижу .</td>\n",
       "      <td>ладно , повесь трубку .</td>\n",
       "      <td>1</td>\n",
       "      <td>нет , звонить буду я .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900968</td>\n",
       "      <td>3</td>\n",
       "      <td>нет звонить буду я</td>\n",
       "      <td>[нет, звонить, буду, я]</td>\n",
       "      <td>нет звонить быть я</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22579918886</td>\n",
       "      <td>кликни на меня а потом на надпись \" видео - зв...</td>\n",
       "      <td>о , я тебя вижу .</td>\n",
       "      <td>ладно , повесь трубку .</td>\n",
       "      <td>2</td>\n",
       "      <td>слушай , я не мог уйти .</td>\n",
       "      <td>0</td>\n",
       "      <td>0.884320</td>\n",
       "      <td>3</td>\n",
       "      <td>слушай я не мог уйти</td>\n",
       "      <td>[слушай, я, не, мог, уйти]</td>\n",
       "      <td>слушай я не мочь уйти</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22579918886</td>\n",
       "      <td>кликни на меня а потом на надпись \" видео - зв...</td>\n",
       "      <td>о , я тебя вижу .</td>\n",
       "      <td>ладно , повесь трубку .</td>\n",
       "      <td>3</td>\n",
       "      <td>я не прекращу звонить .</td>\n",
       "      <td>2</td>\n",
       "      <td>0.982530</td>\n",
       "      <td>3</td>\n",
       "      <td>я не прекращу звонить</td>\n",
       "      <td>[я, не, прекращу, звонить]</td>\n",
       "      <td>я не прекратить звонить</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22579918886</td>\n",
       "      <td>кликни на меня а потом на надпись \" видео - зв...</td>\n",
       "      <td>о , я тебя вижу .</td>\n",
       "      <td>ладно , повесь трубку .</td>\n",
       "      <td>4</td>\n",
       "      <td>я звоню им .</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838054</td>\n",
       "      <td>3</td>\n",
       "      <td>я звоню им</td>\n",
       "      <td>[я, звоню, им]</td>\n",
       "      <td>я звонить имя</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    context_id                                          context_2  \\\n",
       "0  22579918886  кликни на меня а потом на надпись \" видео - зв...   \n",
       "1  22579918886  кликни на меня а потом на надпись \" видео - зв...   \n",
       "2  22579918886  кликни на меня а потом на надпись \" видео - зв...   \n",
       "3  22579918886  кликни на меня а потом на надпись \" видео - зв...   \n",
       "4  22579918886  кликни на меня а потом на надпись \" видео - зв...   \n",
       "\n",
       "           context_1                context_0  reply_id  \\\n",
       "0  о , я тебя вижу .  ладно , повесь трубку .         0   \n",
       "1  о , я тебя вижу .  ладно , повесь трубку .         1   \n",
       "2  о , я тебя вижу .  ладно , повесь трубку .         2   \n",
       "3  о , я тебя вижу .  ладно , повесь трубку .         3   \n",
       "4  о , я тебя вижу .  ладно , повесь трубку .         4   \n",
       "\n",
       "                      reply  label  confidence  context_number  \\\n",
       "0                 не могу .      2    0.875352               3   \n",
       "1    нет , звонить буду я .      1    0.900968               3   \n",
       "2  слушай , я не мог уйти .      0    0.884320               3   \n",
       "3   я не прекращу звонить .      2    0.982530               3   \n",
       "4              я звоню им .      2    0.838054               3   \n",
       "\n",
       "     reply_without_punct                   reply_arr  \\\n",
       "0                не могу                  [не, могу]   \n",
       "1     нет звонить буду я     [нет, звонить, буду, я]   \n",
       "2   слушай я не мог уйти  [слушай, я, не, мог, уйти]   \n",
       "3  я не прекращу звонить  [я, не, прекращу, звонить]   \n",
       "4             я звоню им              [я, звоню, им]   \n",
       "\n",
       "               normal_reply  reply_contains_verb  reply_contains_noun  \\\n",
       "0                  не мочь                     1                    0   \n",
       "1       нет звонить быть я                     1                    0   \n",
       "2    слушай я не мочь уйти                     1                    0   \n",
       "3  я не прекратить звонить                     1                    0   \n",
       "4            я звонить имя                     1                    1   \n",
       "\n",
       "   reply_contains_advb  reply_contains_numr  \n",
       "0                    0                    0  \n",
       "1                    0                    0  \n",
       "2                    0                    0  \n",
       "3                    0                    0  \n",
       "4                    0                    0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_if_verb(row):\n",
    "    f = 0\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if 'VERB' in p.tag:\n",
    "            f =1\n",
    "            \n",
    "    return f\n",
    "\n",
    "def check_if_noun(row):\n",
    "    f = 0\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if 'NOUN' in p.tag:\n",
    "            f =1\n",
    "            \n",
    "    return f\n",
    "\n",
    "def check_if_advb(row):\n",
    "    f = 0\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if 'ADVB' in p.tag:\n",
    "            f =1\n",
    "            \n",
    "    return f \n",
    "\n",
    "def check_if_numr(row):\n",
    "    f = 0\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if 'NUMR' in p.tag:\n",
    "            f =1\n",
    "            \n",
    "    return f\n",
    "\n",
    "train['reply_contains_verb'] = train['reply_arr'].apply(lambda x: check_if_verb(x))\n",
    "test['reply_contains_verb'] = test['reply_arr'].apply(lambda x: check_if_verb(x))\n",
    "\n",
    "train['reply_contains_noun'] = train['reply_arr'].apply(lambda x: check_if_noun(x))\n",
    "test['reply_contains_noun'] = test['reply_arr'].apply(lambda x: check_if_noun(x))\n",
    "\n",
    "train['reply_contains_advb'] = train['reply_arr'].apply(lambda x: check_if_advb(x))\n",
    "test['reply_contains_advb'] = test['reply_arr'].apply(lambda x: check_if_advb(x))\n",
    "\n",
    "train['reply_contains_numr'] = train['reply_arr'].apply(lambda x: check_if_numr(x))\n",
    "test['reply_contains_numr'] = test['reply_arr'].apply(lambda x: check_if_numr(x))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train['reply_gender'] = train['reply_arr'].apply(lambda x: get_gender(x))\\ntest['reply_gender'] = test['reply_arr'].apply(lambda x: check_if_verb(x))\\n\\ntrain['context_0_gender'] = train['context_0_arr'].apply(lambda x: get_gender(x))\\ntest['context_0_gender'] = test['context_0_arr'].apply(lambda x: check_if_verb(x))\\n\\ntrain['context_1_gender'] = train['context_1_arr'].apply(lambda x: get_gender(x))\\ntest['context_1_gender'] = test['context_1_arr'].apply(lambda x: check_if_verb(x))\\n\\ntrain['context_2_gender'] = train['context_2_arr'].apply(lambda x: get_gender(x))\\ntest['context_2_gender'] = test['context_2_arr'].apply(lambda x: check_if_verb(x))\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['context_0_without_punct'] = train['context_0'].apply(lambda x: drop_punctuation(x))\n",
    "train['context_0_without_punct'] = train['context_0_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "train['context_0_arr'] = train['context_0_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "\n",
    "test['context_0_without_punct'] = test['context_0'].apply(lambda x: drop_punctuation(x))\n",
    "test['context_0_without_punct'] = test['context_0_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "test['context_0_arr'] = test['context_0_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "\n",
    "train['context_1_without_punct'] = train['context_1'].apply(lambda x: drop_punctuation(x))\n",
    "train['context_1_without_punct'] = train['context_1_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "train['context_1_arr'] = train['context_1_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "\n",
    "test['context_1_without_punct'] = test['context_1'].apply(lambda x: drop_punctuation(x))\n",
    "test['context_1_without_punct'] = test['context_1_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "test['context_1_arr'] = test['context_1_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "train['context_2_without_punct'] = train['context_2'].apply(lambda x: drop_punctuation(x))\n",
    "train['context_2_without_punct'] = train['context_2_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "train['context_2_arr'] = train['context_2_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "\n",
    "test['context_2_without_punct'] = test['context_2'].apply(lambda x: drop_punctuation(x))\n",
    "test['context_2_without_punct'] = test['context_2_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "test['context_2_arr'] = test['context_2_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "def get_gender(row):\n",
    "    is_masc = 0\n",
    "    is_femn = 0\n",
    "    is_neut = 0\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if p.tag.gender == 'masc':\n",
    "            is_masc = 1\n",
    "        elif p.tag.gender == 'femn':\n",
    "            is_femn = 1\n",
    "        elif p.tag.gender == 'neut':\n",
    "            is_neut = 1  \n",
    "    if is_masc:\n",
    "        return 1\n",
    "    if is_femn:\n",
    "        return 2\n",
    "    if is_neut:\n",
    "        return 3\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['context_0_without_punct'] = final['context_0'].apply(lambda x: drop_punctuation(x))\n",
    "final['context_0_without_punct'] = final['context_0_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "final['context_0_arr'] = final['context_0_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "final['context_1_without_punct'] = final['context_1'].apply(lambda x: drop_punctuation(x))\n",
    "final['context_1_without_punct'] = final['context_1_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "final['context_1_arr'] = final['context_1_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "final['context_2_without_punct'] = final['context_2'].apply(lambda x: drop_punctuation(x))\n",
    "final['context_2_without_punct'] = final['context_2_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "final['context_2_arr'] = final['context_2_without_punct'].str.split(\" \").tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_punctuation(x):\n",
    "    '''\n",
    "        x : string\n",
    "            String from which the punctuation is removed\n",
    "    '''\n",
    "    return re.sub('[%s]' % string.punctuation, ' ', x)\n",
    "\n",
    "def get_normal_form(row):\n",
    "    s = ''\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        s += p.normal_form + ' '\n",
    "    return s\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "train['text'] = train['context_0'] + \" \" + train['context_1'] + \" \" + train['context_2'] \n",
    "test['text'] = test['context_0'] + \" \" + test['context_1'] + \" \" + test['context_2'] \n",
    "\n",
    "train['text_without_punct'] = train['text'].apply(lambda x: drop_punctuation(x))\n",
    "train['text_without_punct'] = train['text_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "train['text_arr'] = train['text_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "\n",
    "test['text_without_punct'] = test['text'].apply(lambda x: drop_punctuation(x))\n",
    "test['text_without_punct'] = test['text_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "test['text_arr'] = test['text_without_punct'].str.split(\" \").tolist()\n",
    "\n",
    "train['normal_text'] = train['text_arr'].apply(lambda x: get_normal_form(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['text'] = final['context_0'] + \" \" + final['context_1'] + \" \" + final['context_2'] \n",
    "\n",
    "\n",
    "final['text_without_punct'] = final['text'].apply(lambda x: drop_punctuation(x))\n",
    "final['text_without_punct'] = final['text_without_punct'].apply(lambda x: ' '.join(x.split()))\n",
    "final['text_arr'] = final['text_without_punct'].str.split(\" \").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['normal_context_0'] = train['context_0_arr'].apply(lambda x: get_normal_form(x))\n",
    "train['normal_context_1'] = train['context_1_arr'].apply(lambda x: get_normal_form(x))\n",
    "train['normal_context_2'] = train['context_2_arr'].apply(lambda x: get_normal_form(x))\n",
    "\n",
    "test['normal_context_0'] = test['context_0_arr'].apply(lambda x: get_normal_form(x))\n",
    "test['normal_context_1'] = test['context_1_arr'].apply(lambda x: get_normal_form(x))\n",
    "test['normal_context_2'] = test['context_2_arr'].apply(lambda x: get_normal_form(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['normal_context_0'] = final['context_0_arr'].apply(lambda x: get_normal_form(x))\n",
    "final['normal_context_1'] = final['context_1_arr'].apply(lambda x: get_normal_form(x))\n",
    "final['normal_context_2'] = final['context_2_arr'].apply(lambda x: get_normal_form(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['normal_reply'] = test['reply_arr'].apply(lambda x: get_normal_form(x))\n",
    "test['normal_text'] = test['text_arr'].apply(lambda x: get_normal_form(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['normal_reply'] = final['reply_arr'].apply(lambda x: get_normal_form(x))\n",
    "final['normal_text'] = final['text_arr'].apply(lambda x: get_normal_form(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case(row):\n",
    "    is_nomn = 0\n",
    "    is_gent = 0\n",
    "    is_datv = 0\n",
    "    is_accs = 0\n",
    "    is_ablt= 0\n",
    "    is_loct = 0\n",
    "    is_voct = 0\n",
    "    is_gen2= 0\n",
    "    is_acc2 = 0\n",
    "    is_loc2 = 0\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]\n",
    "        if p.tag.case == 'nomn':\n",
    "            is_nomn = 1\n",
    "        elif p.tag.case == 'gent':\n",
    "            is_gent = 1\n",
    "        elif p.tag.case == 'datv':\n",
    "            is_datv = 1  \n",
    "        elif p.tag.case == 'accs':\n",
    "            is_accs = 1\n",
    "        elif p.tag.case == 'ablt':\n",
    "            is_ablt = 1 \n",
    "        elif p.tag.case == 'loct':\n",
    "            is_loct = 1\n",
    "        elif p.tag.case == 'voct':\n",
    "            is_voct = 1  \n",
    "        elif p.tag.case == 'gen2':\n",
    "            is_gen2 = 1\n",
    "        elif p.tag.case == 'acc2':\n",
    "            is_acc2 = 1  \n",
    "        elif p.tag.case == 'loc2':\n",
    "            is_loc2 = 1  \n",
    "    if is_nomn:\n",
    "        return 1\n",
    "    if is_gent:\n",
    "        return 2\n",
    "    if is_datv:\n",
    "        return 3\n",
    "    if is_accs:\n",
    "        return 4\n",
    "    if is_ablt:\n",
    "        return 5\n",
    "    if is_loct:\n",
    "        return 6\n",
    "    if is_voct:\n",
    "        return 7\n",
    "    if is_gen2:\n",
    "        return 8\n",
    "    if is_acc2:\n",
    "        return 9\n",
    "    if is_loc2:\n",
    "        return 10\n",
    "    return 0\n",
    "\n",
    "train['reply_case'] = train['reply_arr'].apply(lambda x: get_case(x))\n",
    "test['reply_case'] = test['reply_arr'].apply(lambda x: get_case(x))\n",
    "\n",
    "train['context_0_case'] = train['context_0_arr'].apply(lambda x: get_case(x))\n",
    "test['context_0_case'] = test['context_0_arr'].apply(lambda x: get_case(x))\n",
    "\n",
    "train['context_1_case'] = train['context_1_arr'].apply(lambda x: get_gender(x))\n",
    "test['context_1_case'] = test['context_1_arr'].apply(lambda x: get_case(x))\n",
    "\n",
    "train['context_2_case'] = train['context_2_arr'].apply(lambda x: get_case(x))\n",
    "test['context_2_case'] = test['context_2_arr'].apply(lambda x: get_case(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_text_without_prs(row):\n",
    "    s = ''\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]        \n",
    "        if ('PREP' not in p.tag) and ('CONJ' not in p.tag) and ('PRCL' not in p.tag) and ('INTJ' not in p.tag):\n",
    "            s += p.normal_form + ' '\n",
    "    return s\n",
    "\n",
    "train['normal_reply_without_prs'] = train['reply_arr'].apply(lambda x: get_normal_text_without_prs(x))\n",
    "train['normal_text_without_prs'] = train['text_arr'].apply(lambda x: get_normal_text_without_prs(x))\n",
    "test['normal_reply_without_prs'] = test['reply_arr'].apply(lambda x: get_normal_text_without_prs(x))\n",
    "test['normal_text_without_prs'] = test['text_arr'].apply(lambda x: get_normal_text_without_prs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['normal_reply_without_prs'] = final['reply_arr'].apply(lambda x: get_normal_text_without_prs(x))\n",
    "final['normal_text_without_prs'] = final['text_arr'].apply(lambda x: get_normal_text_without_prs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_without_prs(row):\n",
    "    s = ''\n",
    "    for x in row:\n",
    "        p = morph.parse(x)[0]        \n",
    "        if ('PREP' not in p.tag) and ('CONJ' not in p.tag) and ('PRCL' not in p.tag) and ('INTJ' not in p.tag):\n",
    "            s += x+ ' '\n",
    "    return s\n",
    "\n",
    "train['reply_without_prs'] = train['reply_arr'].apply(lambda x: get_text_without_prs(x))\n",
    "train['text_without_prs'] = train['text_arr'].apply(lambda x: get_text_without_prs(x))\n",
    "test['reply_without_prs'] = test['reply_arr'].apply(lambda x: get_text_without_prs(x))\n",
    "test['text_without_prs'] = test['text_arr'].apply(lambda x: get_text_without_prs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['reply_without_prs'] = final['reply_arr'].apply(lambda x: get_text_without_prs(x))\n",
    "final['text_without_prs'] = final['text_arr'].apply(lambda x: get_text_without_prs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X_train = hstack((X_train, np.array(train['reply_contains_verb'])[:,None]))\\nX_train = hstack((X_train, np.array(train['reply_contains_noun'])[:,None]))\\nX_test = hstack((X_test, np.array(test['reply_contains_verb'])[:,None]))\\nX_test = hstack((X_test, np.array(test['reply_contains_noun'])[:,None]))\\n\\nX_train = hstack((X_train, np.array(train['reply_contains_advb'])[:,None]))\\nX_train = hstack((X_train, np.array(train['reply_contains_numr'])[:,None]))\\nX_test = hstack((X_test, np.array(test['reply_contains_advb'])[:,None]))\\nX_test = hstack((X_test, np.array(test['reply_contains_numr'])[:,None]))\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Vect(df, test):\n",
    "        tfidf = TfidfVectorizer()\n",
    "\n",
    "        tfidf.fit(dt['normal_text'])\n",
    "\n",
    "        context = tfidf.transform(df['normal_text']) \n",
    "        context_t = tfidf.transform(test['normal_text'])\n",
    "\n",
    "        tfidf.fit(dt['normal_reply'])\n",
    "\n",
    "        reply = tfidf.fit_transform(df['normal_reply'])\n",
    "        reply_t = tfidf.transform(test['normal_reply'])\n",
    "\n",
    "        return sps.hstack((context, reply)), \\\n",
    "               sps.hstack((context_t, reply_t))\n",
    "        \n",
    "\n",
    "        \n",
    "def Vect5(train, test):\n",
    "        t1_ft = np.vstack(train['context_0_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "        t2_ft = np.vstack(train['context_1_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "        t3_ft = np.vstack(train['context_2_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "        t5_ft = np.vstack(train['reply_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "\n",
    "        te1_ft = np.vstack(test['context_0_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "        te2_ft = np.vstack(test['context_1_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "        te3_ft = np.vstack(test['context_2_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "        te5_ft = np.vstack(test['reply_without_punct'].apply(lambda x: ft_model.get_sentence_vector(x)))\n",
    "\n",
    "        return np.hstack([t1_ft, t2_ft, t3_ft, t5_ft]), \\\n",
    "                np.hstack([te1_ft, te2_ft, te3_ft, te5_ft])\n",
    "\n",
    "\n",
    "\n",
    "dt = pd.concat([train, test])\n",
    "X_train, X_test = Vect(train, final)\n",
    "X_train5, X_test5 = Vect5(train, final)\n",
    "X_train = hstack((sparse.csr_matrix(X_train), sparse.csr_matrix(X_train5)))\n",
    "X_test = hstack((sparse.csr_matrix(X_test), sparse.csr_matrix(X_test5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train[0:50000]['label'] * train[0:50000]['confidence']\n",
    "clf = LGBMRegressor().fit(X_train, train['target'])\n",
    "y_test = clf.predict(X_test)\n",
    "final['target']  = -y_test\n",
    "final.sort_values(by=['context_id', 'target'])[['context_id', 'reply_id']].to_csv('subm.csv', encoding='utf-8', sep=' ', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
